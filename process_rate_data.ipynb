{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this notebook groups the health insurance rates by state, age and 'metal level', then calculates the mean and standard deviation for each of those groups. \n",
    "\n",
    "The second part calculates the percentage change in rates for each of those groups from year to year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder \n",
    "    .appName(\"Concat Rates\") \n",
    "    .getOrCreate())\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "rates2014 = (spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    "             .load('s3://ryan-afa/input/rates-2014.csv', inferSchema=True, header=True)\n",
    "             .cache())\n",
    "\n",
    "plans2014 = (spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    "             .load('s3://ryan-afa/input/plan-2014.csv', inferSchema=True, header=True)\n",
    "             .cache())\n",
    "rates2017 = (spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    "             .load('s3://ryan-afa/input/rates-2017.csv', inferSchema=True, header=True)\n",
    "             .cache())\n",
    "plans2017 = (spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    "             .load('s3://ryan-afa/input/plan-2017.csv', inferSchema=True, header=True)\n",
    "             .cache())\n",
    "\n",
    "rates2016 = (spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    "             .load('s3://ryan-afa/input/rates-2016.csv', inferSchema=True, header=True)\n",
    "             .cache())\n",
    "plans2016 = (spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    "             .load('s3://ryan-afa/input/plan-2016.csv', inferSchema=True, header=True)\n",
    "             .cache())\n",
    "\n",
    "rates2015 = (spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    "             .load('s3://ryan-afa/input/rates-2015.csv', inferSchema=True, header=True)\n",
    "             .cache())\n",
    "plans2015 = (spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    "             .load('s3://ryan-afa/input/plan-2015.csv', inferSchema=True, header=True)\n",
    "             .cache())\n",
    "\n",
    "\n",
    "\n",
    "def getMeans(rates, plans):\n",
    "    \n",
    "    #missing rate data was filled with 9999 -- so we get rid of those rows \n",
    "    rates = rates.filter(rates.IndividualRate  < 9999)\n",
    "    \n",
    "    # to get the metal level for each plan, we need to join the rates data\n",
    "    # the plan attributes data set. But first we reformat the planId column\n",
    "    # so we can do the join, filter out dental plans, and get unique combinations\n",
    "    # of the variables we're interested in.\n",
    "    plans = plans.withColumn('PlanId', substring_index(plans.PlanId, '-', 1))\n",
    "    plans = plans.filter(plans.DentalOnlyPlan != \"Yes\")\n",
    "    plans = plans.dropDuplicates(['PlanId', 'StateCode', 'MetalLevel'])\n",
    "    \n",
    "    #join the data sets\n",
    "    merged = rates.join(plans.select('StateCode', 'PlanId', 'MetalLevel'), on=['StateCode', 'PlanId'])\n",
    "    \n",
    "    #compute the means and standard deviations \n",
    "    means = (merged.groupby(['Age', 'MetalLevel', 'StateCode'])\n",
    "             .agg(mean(merged.IndividualRate), stddev(merged.IndividualRate)))\n",
    "    \n",
    "    #rename the columns we just computed so they're friendlier. \n",
    "    return (means\n",
    "            .withColumnRenamed('avg(IndividualRate)', 'avg_IndividualRate')\n",
    "            .withColumnRenamed('stddev_samp(IndividualRate', 'std_IndividualRate')).cache()\n",
    "\n",
    "merged2014 = getMeans(rates2014, plans2014)\n",
    "merged2017 = getMeans(rates2017, plans2017)\n",
    "merged2015 = getMeans(rates2015, plans2015)\n",
    "merged2016 = getMeans(rates2016, plans2016)\n",
    "\n",
    "(merged2017\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    " .save('s3://ryan-afa/output/averages-2017.csv',  header=True))\n",
    "\n",
    "(merged2016\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    " .save('s3://ryan-afa/ouput/averages-2016.csv',  header=True))\n",
    "\n",
    "(merged2014\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    " .save('s3://ryan-afa/output/averages-2014.csv',  header=True))\n",
    "\n",
    "(merged2015\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    " .save('s3://ryan-afa/output/averages-2015.csv',  header=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPercChange(rates1, rates2, suffixes):\n",
    "    \n",
    "\n",
    "    #add suffixes to the column names so we can distinguish them after we join the dataframes\n",
    "    firstYearAvg =  'avg_IndividualRate' + suffixes[0]\n",
    "    secondYearAvg = 'avg_IndividualRate' + suffixes[1]\n",
    "    firstYearStd = 'std_IndividualRate' +  suffixes[0]\n",
    "    secondYearStd = 'std_IndividualRate' +  suffixes[1]\n",
    "    rates1 = (rates1.withColumnRenamed('avg_IndividualRate', firstYearAvg)\n",
    "              .withColumnRenamed('stddev_samp(IndividualRate)', firstYearStd))\n",
    "    rates2 = (rates2.withColumnRenamed('avg_IndividualRate', secondYearAvg)\n",
    "             .withColumnRenamed('stddev_samp(IndividualRate)', secondYearStd))\n",
    "    \n",
    "    #join the rates for consecutive years\n",
    "    ratesJoined = rates1.join(rates2, on=['StateCode', 'Age', 'MetalLevel'])\n",
    "    \n",
    "    #for each rate column, calculate the percentage change in rate between the years, and add it to the dataframe. \n",
    "    ratesJoined = ratesJoined.withColumn(\n",
    "            'percentChange', ((ratesJoined[secondYearAvg] - ratesJoined[firstYearAvg]) / ratesJoined[firstYearAvg]) * 100)\n",
    "    return ratesJoined\n",
    "\n",
    "\n",
    "change1415 = getPercChange(merged2014, merged2015, ['_2014', '_2015'])\n",
    "change1516 = getPercChange(merged2015, merged2016, ['_2015', '_2016'])\n",
    "change1617 = getPercChange(merged2016, merged2017, ['_2016', '_2017'])\n",
    "\n",
    "\n",
    "(change1415\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    " .save('s3://ryan-afa/output/changes-1415.csv',  header=True))\n",
    "\n",
    "(change1516\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    " .save('s3://ryan-afa/ouput/changes-1516.csv',  header=True))\n",
    "\n",
    "(change1617\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\n",
    " .save('s3://ryan-afa/output/changes-1617.csv',  header=True))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.12\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
